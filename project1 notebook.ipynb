{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note: this is just a template for PA 1 and the code is for references only.\n",
    "Feel free to design the pipeline of the *main* function. However, one should keep\n",
    "the interfaces for the other functions unchanged. Change the returned values of\n",
    "these functions so that they are consistent with the assignment instructions.\n",
    "In general, one will only need to add the code below the TO-DO statements to\n",
    "finish the assignment. Additional import statements can be included when needed.\n",
    "\n",
    "For the kNN classifier, one could use existing libraries to compute the pairwise\n",
    "Euclidean distances between the test and training data, as for-loops in Python\n",
    "are pretty slow. Other than that, the designs of all functions should be your\n",
    "original work.\n",
    "'''\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(test_y, pred_y):\n",
    "\n",
    "    # TO-DO: add your code here\n",
    "\n",
    "    return None\n",
    "\n",
    "def test_knn(train_x, train_y, test_x, num_nn):\n",
    "\n",
    "    # TO-DO: add your code here\n",
    "\n",
    "    return None\n",
    "\n",
    "def test_pocket(w, test_x):\n",
    "\n",
    "    # TO-DO: add your code here\n",
    "\n",
    "    return None\n",
    "\n",
    "def train_pocket(train_x, train_y, num_iters):\n",
    "\n",
    "    # TO-DO: add your code here\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_id():\n",
    "\n",
    "    return 'tuk01401'\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Read the data file\n",
    "    szDatasetPath = './letter-recognition.data' # Put this file in the same place as this script\n",
    "    listClasses = []\n",
    "    listAttrs = []\n",
    "    with open(szDatasetPath) as csvFile:\n",
    "        csvReader = csv.reader(csvFile, delimiter=',')\n",
    "        for row in csvReader:\n",
    "            listClasses.append(row[0])\n",
    "            listAttrs.append(list(map(float, row[1:])))\n",
    "\n",
    "    # Generate the mapping from class name to integer IDs\n",
    "    mapCls2Int = dict([(y, x) for x, y in enumerate(sorted(set(listClasses)))])\n",
    "\n",
    "    # Store the dataset with numpy array\n",
    "    dataX = np.array(listAttrs)\n",
    "    dataY = np.array([mapCls2Int[cls] for cls in listClasses])\n",
    "\n",
    "    # Split the dataset as the training set and test set\n",
    "    nNumTrainingExamples = 15000\n",
    "    trainX = dataX[:nNumTrainingExamples, :]\n",
    "    trainY = dataY[:nNumTrainingExamples]\n",
    "    testX = dataX[nNumTrainingExamples:, :]\n",
    "    testY = dataY[nNumTrainingExamples:]\n",
    "\n",
    "    # TO-DO: add your code here\n",
    "\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Read the data file\n",
    "szDatasetPath = './letter-recognition.data' # Put this file in the same place as this script\n",
    "listClasses = []\n",
    "listAttrs = []\n",
    "with open(szDatasetPath) as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter=',')\n",
    "    for row in csvReader:\n",
    "        listClasses.append(row[0])\n",
    "        listAttrs.append(list(map(float, row[1:])))\n",
    "\n",
    "# Generate the mapping from class name to integer IDs\n",
    "mapCls2Int = dict([(y, x) for x, y in enumerate(sorted(set(listClasses)))])\n",
    "\n",
    "# Store the dataset with numpy array\n",
    "dataX = np.array(listAttrs)\n",
    "dataY = np.array([mapCls2Int[cls] for cls in listClasses])\n",
    "\n",
    "# Split the dataset as the training set and test set\n",
    "nNumTrainingExamples = 15000\n",
    "trainX = dataX[:nNumTrainingExamples, :]\n",
    "trainY = dataY[:nNumTrainingExamples]\n",
    "testX = dataX[nNumTrainingExamples:, :]\n",
    "testY = dataY[nNumTrainingExamples:]\n",
    "\n",
    "# TO-DO: add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the KNN operator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_knn(train_x, train_y, test_x, num_nn)\n",
    "num_nn = 1\n",
    "\n",
    "#TODO can we assume the matrix shapes will match?\n",
    "neighb_dict = {}\n",
    "squared = 0\n",
    "pred_labels = np.empty([testX.shape[0],], dtype=int)\n",
    "\n",
    "#Go through every test point\n",
    "for i in range(0, 50):\n",
    "    #Go through every training point\n",
    "    for j in range(0, trainX.shape[0]):\n",
    "        #Find the disctance between the test point and training point\n",
    "        for k in range(0, trainX.shape[1]):\n",
    "            squared += ((testX[i,k] - trainX[j,k])**2)\n",
    "        neighb_dict[j] = (squared**.5)\n",
    "        squared = 0\n",
    "    #Sort the dictionary by the values not keys (taken from most common stackoverflow answer) \n",
    "    dict_sorted = {k: v for k, v in sorted(neighb_dict.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    keys_list = list(dict_sorted)\n",
    "    nearest_labels = {}\n",
    "    \n",
    "    for l in range(0, num_nn):\n",
    "        if trainY[keys_list[l]] in nearest_labels:\n",
    "            nearest_labels[trainY[keys_list[l]]] += 1\n",
    "        else:\n",
    "            nearest_labels[trainY[keys_list[l]]] = 1\n",
    "    #TODO Can I assume the top label is \"random\" from the sorting in the case of ties\n",
    "    labels_sorted = {k: v for k, v in sorted(nearest_labels.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    #TODO I think these are coming out as floats\n",
    "    pred_labels[i] = list(labels_sorted.keys())[0]\n",
    "\n",
    "#print(pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Pocket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def compute_accuracy(test_y, pred_x):\n",
    "\n",
    "    # Returns an array with true at every match\n",
    "    correct_labels = (test_y == pred_y)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(0, test_y.shape[0]):\n",
    "        if (correct_labels == True):\n",
    "            count += 1\n",
    "    accuracy = count / test_y.shape[0]\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def test_pocket(w, test_x):\n",
    "\n",
    "    # Add the extra dimension to the testing features to account for the b\n",
    "    extra_dim = np.ones((test_X.shape[0],1), dtype=int)\n",
    "    test_X_mod = np.append(test_X, extra_dim, axis=1)\n",
    "    \n",
    "    # Create arrays for the guesses of each letter, and the final prediction\n",
    "    dot_prods = np.empty(26,)\n",
    "    preds = np.empty(test_x.shape[0])\n",
    "\n",
    "    # Go through each testing point and test it for every letter.  Prediction is the highest scoring\n",
    "    for i in range (0, test_X_mod.shape[0]):\n",
    "        for j in range (0, 26):\n",
    "            dot_prods[j] = np.dot(w[j], test_X_mod[i])\n",
    "        preds[i] = np.argnax(dot_prods)\n",
    "        \n",
    "    return preds\n",
    "\n",
    "#Add the extra dimension here to make it easier to get the dot product later (weights have b included)\n",
    "extra_dim = np.ones((trainX.shape[0],1), dtype=int)\n",
    "trainX_mod = np.append(trainX, extra_dim, axis=1)\n",
    "\n",
    "#A vector of weights for each dimension and the 1 extra, and k labels worth of weights\n",
    "weights = np.zeros([26,trainX_mod.shape[1]])\n",
    "\n",
    "num_iters = 1\n",
    "preds = np.empty_like(trainY)\n",
    "misclassified_list = []\n",
    "\n",
    "for i in range(0, num_iters):\n",
    "    for j in range(0, 26):\n",
    "        for k in range(0, trainX.shape[0]):\n",
    "            preds[k] = np.dot(weights[j], trainX_mod[k])\n",
    "            if (((trainY[k] == j) and (preds[k] <= 0)) or ((trainY[k] != j) and (preds[k] > 0))):\n",
    "                misclassified_list.append(k)\n",
    "        point_to_update = random.choice(misclassified_list)\n",
    "        if (trainY[point_to_update] == j):\n",
    "            weights[j] += trainX_mod[point_to_update]\n",
    "        else:\n",
    "            weights[j] -= trainX_mod[point_to_update] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 78, 118, 130, 134, 144, 147, 157, 163, 216, 232, 234, 289, 295, 304, 313, 327, 337, 415, 473, 483, 502, 517, 608, 638, 675, 686, 719, 732, 749, 769, 789, 790, 813, 892, 944, 958, 991, 1014, 1023, 1027, 1030, 1041, 1042, 1062, 1077, 1139, 1227, 1264, 1299, 1325, 1329, 1349, 1387, 1396, 1435, 1439, 1442, 1475, 1492, 1504, 1532, 1560, 1593, 1603, 1622, 1692, 1699, 1700, 1701, 1715, 1779, 1824, 1856, 1865, 1901, 1950, 1953, 1963, 1994, 1999, 2022, 2026, 2072, 2180, 2204, 2211, 2218, 2231, 2237, 2249, 2250, 2264, 2283, 2307, 2362, 2372, 2404, 2407, 2441, 2449, 2455, 2469, 2490, 2494, 2500, 2511, 2539, 2544, 2545, 2562, 2576, 2601, 2625, 2638, 2659, 2706, 2741, 2743, 2746, 2751, 2755, 2800, 2801, 2814, 2866, 2898, 2920, 2937, 2985, 2988, 2995, 3001, 3025, 3045, 3058, 3089, 3094, 3138, 3143, 3224, 3360, 3467, 3478, 3509, 3556, 3606, 3647, 3661, 3702, 3732, 3755, 3757, 3771, 3782, 3785, 3802, 3811, 3843, 3953, 4034, 4067, 4079, 4100, 4106, 4113, 4137, 4186, 4225, 4242, 4245, 4257, 4301, 4360, 4381, 4384, 4401, 4417, 4419, 4429, 4483, 4508, 4575, 4639, 4710, 4795, 4830, 4840, 4841, 4859, 4886, 4899, 4947, 4958, 5018, 5040, 5043, 5044, 5077, 5094, 5108, 5123, 5149, 5158, 5194, 5220, 5230, 5241, 5243, 5271, 5325, 5366, 5380, 5403, 5415, 5422, 5424, 5535, 5550, 5613, 5624, 5630, 5682, 5687, 5710, 5748, 5752, 5783, 5822, 5853, 5856, 5873, 5884, 5890, 5942, 6117, 6127, 6129, 6137, 6167, 6169, 6252, 6306, 6308, 6378, 6408, 6412, 6451, 6452, 6526, 6527, 6562, 6661, 6687, 6688, 6751, 6759, 6791, 6809, 6896, 6902, 6930, 6950, 6958, 6977, 6985, 7005, 7019, 7042, 7047, 7130, 7180, 7194, 7221, 7247, 7252, 7299, 7300, 7336, 7374, 7389, 7398, 7406, 7412, 7425, 7426, 7433, 7436, 7470, 7494, 7528, 7538, 7566, 7582, 7608, 7612, 7641, 7657, 7691, 7693, 7696, 7746, 7749, 7752, 7814, 7853, 7854, 7898, 7931, 7937, 8001, 8002, 8017, 8029, 8043, 8070, 8113, 8118, 8141, 8147, 8165, 8200, 8212, 8223, 8248, 8252, 8255, 8266, 8269, 8276, 8284, 8313, 8320, 8341, 8378, 8400, 8427, 8428, 8575, 8581, 8605, 8628, 8665, 8668, 8688, 8726, 8752, 8806, 8825, 8858, 8961, 8977, 8984, 8997, 9005, 9026, 9060, 9063, 9065, 9083, 9097, 9140, 9148, 9173, 9180, 9198, 9236, 9237, 9338, 9345, 9384, 9394, 9409, 9410, 9433, 9443, 9539, 9560, 9573, 9580, 9593, 9617, 9643, 9687, 9770, 9778, 9826, 9861, 9863, 9891, 9917, 9961, 9966, 10020, 10024, 10062, 10183, 10203, 10220, 10247, 10289, 10316, 10401, 10417, 10474, 10520, 10527, 10541, 10569, 10578, 10667, 10671, 10675, 10706, 10729, 10767, 10768, 10830, 10835, 10844, 10855, 10882, 10888, 10905, 10961, 11042, 11066, 11069, 11094, 11108, 11128, 11143, 11162, 11176, 11186, 11219, 11244, 11272, 11344, 11407, 11448, 11531, 11574, 11606, 11621, 11646, 11648, 11655, 11690, 11708, 11722, 11757, 11764, 11773, 11799, 11808, 11850, 11876, 11879, 12002, 12010, 12028, 12060, 12065, 12070, 12090, 12102, 12105, 12127, 12146, 12199, 12241, 12252, 12266, 12325, 12336, 12342, 12389, 12455, 12487, 12505, 12508, 12541, 12572, 12632, 12674, 12686, 12703, 12704, 12708, 12718, 12743, 12748, 12797, 12854, 12855, 12880, 12885, 12893, 12912, 12936, 13013, 13032, 13063, 13095, 13125, 13149, 13161, 13181, 13259, 13271, 13273, 13295, 13303, 13316, 13381, 13398, 13414, 13463, 13467, 13474, 13484, 13505, 13538, 13566, 13581, 13585, 13616, 13619, 13659, 13690, 13699, 13702, 13725, 13763, 13820, 13853, 13886, 13889, 13911, 13925, 13941, 13948, 13971, 14015, 14032, 14154, 14188, 14198, 14222, 14256, 14328, 14349, 14353, 14361, 14379, 14385, 14405, 14431, 14436, 14447, 14489, 14505, 14514, 14538, 14539, 14564, 14643, 14657, 14674, 14693, 14703, 14717, 14759, 14773, 14774, 14805, 14848, 14852, 14871, 14874, 14937, 14952]\n"
     ]
    }
   ],
   "source": [
    "print(misclassified_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250\n",
      "[[ 2.  8.  4.  6.  2. 12.  3.  3.  2. 10.  1.  9.  2.  6.  2.  8.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(point_to_update)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
